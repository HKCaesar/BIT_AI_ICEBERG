{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import *\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, ZeroPadding2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "data = pd.read_json('train.json')\n",
    "\n",
    "data = data.drop('id', 1)\n",
    "data = data.drop('inc_angle', 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = data['is_iceberg']\n",
    "\n",
    "data = data.drop('is_iceberg', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = data['band_1']\n",
    "X2 = data['band_2']\n",
    "X1 = np.array(X1)\n",
    "X2 = np.array(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for i in range(len(X1)):\n",
    "    X.extend(X1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1604, 75, 75, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "X = np.reshape(X, (len(X1), 75, 75, 1))\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1604, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = to_categorical(Y)\n",
    "\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1300, 75, 75, 1)\n",
      "(304, 75, 75, 1)\n",
      "(1300, 2)\n",
      "(304, 2)\n"
     ]
    }
   ],
   "source": [
    "image_height = 75\n",
    "image_width = 75\n",
    "\n",
    "train_samples = 1300\n",
    "validation_samples = 304\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "number_of_classes = 2\n",
    "\n",
    "X_train = X[:train_samples, :, : ,:]\n",
    "X_train = abs(X_train/(np.max(X_train) - np.min(X_train)))\n",
    "X_test = X[train_samples:, :, :, :]\n",
    "X_test = abs(X_test/(np.max(X_test) - np.min(X_test)))\n",
    "Y_train = Y[:train_samples, :]\n",
    "Y_test = Y[train_samples:, :]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# sample_image = (X_train[9])\n",
    "\n",
    "# sample_image = np.reshape(elo, (75,75))\n",
    "# plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_width, image_height, 1)))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model1.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model1.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model1.add(Dense(512, activation='relu', use_bias=True, bias_initializer='zeros'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model1.add(Dense(number_of_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1300 samples, validate on 304 samples\n",
      "Epoch 1/20\n",
      "1300/1300 [==============================] - 52s - loss: 0.7186 - acc: 0.4931 - val_loss: 0.6561 - val_acc: 0.5921\n",
      "Epoch 2/20\n",
      "1300/1300 [==============================] - 51s - loss: 0.6319 - acc: 0.6215 - val_loss: 0.6570 - val_acc: 0.4770\n",
      "Epoch 3/20\n",
      "1300/1300 [==============================] - 51s - loss: 0.5882 - acc: 0.6800 - val_loss: 0.6697 - val_acc: 0.5296\n",
      "Epoch 4/20\n",
      "1300/1300 [==============================] - 52s - loss: 0.5293 - acc: 0.7354 - val_loss: 0.5920 - val_acc: 0.6349\n",
      "Epoch 5/20\n",
      "1300/1300 [==============================] - 51s - loss: 0.5055 - acc: 0.7608 - val_loss: 0.6559 - val_acc: 0.6184\n",
      "Epoch 6/20\n",
      "1300/1300 [==============================] - 54s - loss: 0.5084 - acc: 0.7500 - val_loss: 0.6052 - val_acc: 0.6349\n",
      "Epoch 7/20\n",
      "1300/1300 [==============================] - 55s - loss: 0.4254 - acc: 0.8008 - val_loss: 0.5969 - val_acc: 0.6546\n",
      "Epoch 8/20\n",
      "1300/1300 [==============================] - 52s - loss: 0.4025 - acc: 0.8085 - val_loss: 0.5369 - val_acc: 0.7204\n",
      "Epoch 9/20\n",
      "1300/1300 [==============================] - 52s - loss: 0.3426 - acc: 0.8469 - val_loss: 0.4532 - val_acc: 0.7533\n",
      "Epoch 10/20\n",
      "1300/1300 [==============================] - 52s - loss: 0.3137 - acc: 0.8515 - val_loss: 0.4061 - val_acc: 0.7895\n",
      "Epoch 11/20\n",
      "1300/1300 [==============================] - 55s - loss: 0.3303 - acc: 0.8492 - val_loss: 0.4268 - val_acc: 0.7961\n",
      "Epoch 12/20\n",
      "1300/1300 [==============================] - 52s - loss: 0.2434 - acc: 0.8900 - val_loss: 0.5126 - val_acc: 0.7204\n",
      "Epoch 13/20\n",
      "1300/1300 [==============================] - 52s - loss: 0.2428 - acc: 0.8877 - val_loss: 0.4325 - val_acc: 0.8224\n",
      "Epoch 14/20\n",
      "1300/1300 [==============================] - 51s - loss: 0.2513 - acc: 0.8892 - val_loss: 0.5782 - val_acc: 0.7270\n",
      "Epoch 15/20\n",
      "1300/1300 [==============================] - 50s - loss: 0.1912 - acc: 0.9223 - val_loss: 0.4049 - val_acc: 0.8553\n",
      "Epoch 16/20\n",
      "1300/1300 [==============================] - 51s - loss: 0.1971 - acc: 0.9092 - val_loss: 0.5329 - val_acc: 0.7500\n",
      "Epoch 17/20\n",
      "1300/1300 [==============================] - 50s - loss: 0.2038 - acc: 0.9215 - val_loss: 0.6169 - val_acc: 0.7171\n",
      "Epoch 18/20\n",
      "1300/1300 [==============================] - 51s - loss: 0.1286 - acc: 0.9438 - val_loss: 0.9672 - val_acc: 0.7434\n",
      "Epoch 19/20\n",
      "1300/1300 [==============================] - 50s - loss: 0.1132 - acc: 0.9500 - val_loss: 1.0100 - val_acc: 0.7336\n",
      "Epoch 20/20\n",
      "1300/1300 [==============================] - 51s - loss: 0.0901 - acc: 0.9638 - val_loss: 0.7354 - val_acc: 0.7796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5aba07ec50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, Y_train,\n",
    "              batch_size=batch_size, validation_data=(X_test, Y_test),\n",
    "              epochs=epochs,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1.save('100_acc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Yhat = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(Yhat)):\n",
    "    Yhat[i, :] = np.round(Yhat[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.sum(np.all(Yhat == Y_test, axis=1))/len(Yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_json('test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id']=test['id']\n",
    "submission['is_iceberg']=preds\n",
    "submission.to_csv('sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
