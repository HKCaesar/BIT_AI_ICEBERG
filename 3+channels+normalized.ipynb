{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "# -----\n",
    "from scipy import signal\n",
    "from skimage.restoration import denoise_tv_chambolle\n",
    "from skimage.filters import gaussian\n",
    "import json\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, ZeroPadding2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def channels(data):\n",
    "    rgb_arrays = []\n",
    "    for i, row in data.iterrows():\n",
    "        \n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = (band_1 + band_2)/2\n",
    "        \n",
    "        band_1 = denoise_tv_chambolle(band_1, weight=0.05) # chosen arbitrarily\n",
    "        band_2 = denoise_tv_chambolle(band_2, weight=0.05)\n",
    "        band_3 = denoise_tv_chambolle(band_3, weight=0.05)\n",
    "        \n",
    "        # gradient magnitude - doesn't seem to work any better than (b1 + b2)/2\n",
    "#         xder = np.array([[-1,0,1],[-2,0,2],[-1,0,1]])\n",
    "#         yder = np.array([[1,2,1],[0,0,0],[-1,-2,-1]])\n",
    "#         arrx = signal.convolve2d(band_1, xder, mode='same')\n",
    "#         arry = signal.convolve2d(band_1, yder, mode='same')\n",
    "#         band_3 = np.hypot(arrx, arry) \n",
    "\n",
    "        rgb = np.dstack((band_1, band_2, band_3))\n",
    "        rgb_arrays.append(rgb)\n",
    "        \n",
    "    rgb_arrays = np.array(rgb_arrays)\n",
    "    for j in range(3):\n",
    "        rgb_arrays[:, :, :, j] = (rgb_arrays[:, :, :, j] - np.mean(rgb_arrays[:, :, :, j]))/np.std(rgb_arrays[:, :, :, j])\n",
    "    return np.array(rgb_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_json('train.json')\n",
    "Y = X['is_iceberg']  # labels\n",
    "\n",
    "X = channels(X)\n",
    "Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1363, 75, 75, 3)\n",
      "(241, 75, 75, 3)\n",
      "(1363, 2)\n",
      "(241, 2)\n"
     ]
    }
   ],
   "source": [
    "image_height = 75\n",
    "image_width = 75\n",
    "channels = 3\n",
    "number_of_classes = 2\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "# 1\n",
    "model1.add(Conv2D(32, (3, 3), input_shape=(image_width, image_height, channels)))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 2\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(64, (3, 3)))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 3\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(128, (3, 3)))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 4\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(256, (3, 3)))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model1.add(Flatten())\n",
    "\n",
    "# 0\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(512, use_bias=True, bias_initializer='zeros'))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(Dropout(0.15))\n",
    "\n",
    "# 1\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(256, use_bias=True, bias_initializer='zeros'))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(Dropout(0.15))\n",
    "\n",
    "# 2\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(128, use_bias=True, bias_initializer='zeros'))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(Dropout(0.15))\n",
    "\n",
    "# 3\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(64, use_bias=True, bias_initializer='zeros'))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(Dropout(0.15))\n",
    "\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(number_of_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1363 samples, validate on 241 samples\n",
      "Epoch 1/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.7165 - acc: 0.6376 - val_loss: 0.6890 - val_acc: 0.6017\n",
      "Epoch 2/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.5731 - acc: 0.7256 - val_loss: 0.6523 - val_acc: 0.6639\n",
      "Epoch 3/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.5127 - acc: 0.7638 - val_loss: 0.6072 - val_acc: 0.7137\n",
      "Epoch 4/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.4794 - acc: 0.7733 - val_loss: 0.3908 - val_acc: 0.8340\n",
      "Epoch 5/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.4345 - acc: 0.7887 - val_loss: 0.3056 - val_acc: 0.8672\n",
      "Epoch 6/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.4014 - acc: 0.8202 - val_loss: 0.3508 - val_acc: 0.8465\n",
      "Epoch 7/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.4450 - acc: 0.7858 - val_loss: 0.2798 - val_acc: 0.8797\n",
      "Epoch 8/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.3591 - acc: 0.8386 - val_loss: 0.2277 - val_acc: 0.9004\n",
      "Epoch 9/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.3819 - acc: 0.8291 - val_loss: 0.3414 - val_acc: 0.8548\n",
      "Epoch 10/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.3533 - acc: 0.8415 - val_loss: 0.3168 - val_acc: 0.8548\n",
      "Epoch 11/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.3242 - acc: 0.8650 - val_loss: 0.2808 - val_acc: 0.8838\n",
      "Epoch 12/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.2960 - acc: 0.8731 - val_loss: 0.2319 - val_acc: 0.9046\n",
      "Epoch 13/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.2909 - acc: 0.8731 - val_loss: 0.2834 - val_acc: 0.8838\n",
      "Epoch 14/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.2580 - acc: 0.8892 - val_loss: 0.2440 - val_acc: 0.9170\n",
      "Epoch 15/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.2622 - acc: 0.9002 - val_loss: 0.2587 - val_acc: 0.8589\n",
      "Epoch 16/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.2028 - acc: 0.9178 - val_loss: 0.2931 - val_acc: 0.8838\n",
      "Epoch 17/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.3262 - acc: 0.8599 - val_loss: 0.3330 - val_acc: 0.8548\n",
      "Epoch 18/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.2722 - acc: 0.8841 - val_loss: 0.2940 - val_acc: 0.8755\n",
      "Epoch 19/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.2476 - acc: 0.8995 - val_loss: 0.4045 - val_acc: 0.8216\n",
      "Epoch 20/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.1795 - acc: 0.9362 - val_loss: 0.3250 - val_acc: 0.8838\n",
      "Epoch 21/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.1809 - acc: 0.9252 - val_loss: 0.2272 - val_acc: 0.9129\n",
      "Epoch 22/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.1465 - acc: 0.9420 - val_loss: 0.3301 - val_acc: 0.8755\n",
      "Epoch 23/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.2266 - acc: 0.9046 - val_loss: 0.2095 - val_acc: 0.9212\n",
      "Epoch 24/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.2242 - acc: 0.9149 - val_loss: 0.2737 - val_acc: 0.8921\n",
      "Epoch 25/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.1620 - acc: 0.9406 - val_loss: 0.2958 - val_acc: 0.8714\n",
      "Epoch 26/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.1488 - acc: 0.9428 - val_loss: 0.3025 - val_acc: 0.8838\n",
      "Epoch 27/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.1376 - acc: 0.9479 - val_loss: 0.2591 - val_acc: 0.8880\n",
      "Epoch 28/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.1435 - acc: 0.9523 - val_loss: 0.2440 - val_acc: 0.8880\n",
      "Epoch 29/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.1125 - acc: 0.9574 - val_loss: 0.3996 - val_acc: 0.8672\n",
      "Epoch 30/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.1280 - acc: 0.9538 - val_loss: 0.3355 - val_acc: 0.8755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f590dabd898>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, Y_train,\n",
    "              batch_size=batch_size, validation_data=(X_test, Y_test),\n",
    "              epochs=epochs,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reaches ~90% fairly quickly, then becomes a bit unstable :/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model1.predict(X_test)\n",
    "for i in range(len(Y_pred)):\n",
    "    Y_pred[i, :] = np.round(Y_pred[i, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AC:0.89 PR:0.82 RE:0.94 F1:0.88\n"
     ]
    }
   ],
   "source": [
    "#Precision: how many 1 are true 1?\n",
    "#Recall: how many 0 are true 0? \n",
    "#F1Score: balance between those two\n",
    "\n",
    "y_comp = confusion_matrix(Y_test[:, 1], Y_pred[:, 1])\n",
    "precision = y_comp[1, 1] / (y_comp[0, 1] + y_comp[1, 1])\n",
    "recall = y_comp[1, 1] / (y_comp[1, 0] + y_comp[1, 1])\n",
    "accuracy = accuracy_score(Y_test[:, 1], Y_pred[:, 1])\n",
    "f1score = 2 * precision * recall / (precision + recall)\n",
    "print(\"AC:%.2f PR:%.2f RE:%.2f F1:%.2f\" % (accuracy, precision, recall, f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = (\"model_%i_%i_%i_%i.h5\" % (accuracy*100, precision*100, recall*100, f1score*100))\n",
    "filename2 = (\"model_%i_%i_%i_%i.json\" % (accuracy*100, precision*100, recall*100, f1score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = model1.to_json()\n",
    "with open(filename2, \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
