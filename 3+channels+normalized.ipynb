{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "# -----\n",
    "from scipy import signal\n",
    "from skimage.restoration import denoise_tv_chambolle\n",
    "from skimage.filters import gaussian\n",
    "import json\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, ZeroPadding2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def channels(data):\n",
    "    rgb_arrays = []\n",
    "    for i, row in data.iterrows():\n",
    "        \n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = (band_1 + band_2)/2\n",
    "        \n",
    "        band_1 = denoise_tv_chambolle(band_1, weight=0.25) # chosen arbitrarily\n",
    "        band_2 = denoise_tv_chambolle(band_2, weight=0.25)\n",
    "        band_3 = denoise_tv_chambolle(band_3, weight=0.25)\n",
    "        \n",
    "        # gradient magnitude - doesn't seem to work any better than (b1 + b2)/2\n",
    "#         xder = np.array([[-1,0,1],[-2,0,2],[-1,0,1]])\n",
    "#         yder = np.array([[1,2,1],[0,0,0],[-1,-2,-1]])\n",
    "#         arrx = signal.convolve2d(band_1, xder, mode='same')\n",
    "#         arry = signal.convolve2d(band_1, yder, mode='same')\n",
    "#         band_3 = np.hypot(arrx, arry) \n",
    "\n",
    "        rgb = np.dstack((band_1, band_2, band_3))\n",
    "        rgb_arrays.append(rgb)\n",
    "        \n",
    "    rgb_arrays = np.array(rgb_arrays)\n",
    "    return np.array(rgb_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_json('train.json')\n",
    "Y = X['is_iceberg']  # labels\n",
    "\n",
    "# X = channels(X)\n",
    "Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1363, 75, 75, 3)\n",
      "(241, 75, 75, 3)\n",
      "(1363, 2)\n",
      "(241, 2)\n"
     ]
    }
   ],
   "source": [
    "image_height = 75\n",
    "image_width = 75\n",
    "channels_num = 3\n",
    "number_of_classes = 2\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)\n",
    "\n",
    "X_train = channels(X_train)\n",
    "X_test = channels(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "# 1\n",
    "model1.add(Conv2D(32, (3, 3), input_shape=(image_width, image_height, channels_num)))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 2\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(64, (3, 3)))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 3\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(128, (3, 3)))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model1.add(Flatten())\n",
    "\n",
    "# 1\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(256, use_bias=True, bias_initializer='zeros'))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(Dropout(0.3))\n",
    "\n",
    "# 2\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(128, use_bias=True, bias_initializer='zeros'))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(Dropout(0.3))\n",
    "\n",
    "# 3\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(64, use_bias=True, bias_initializer='zeros'))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(Dropout(0.3))\n",
    "\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(number_of_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 73, 73, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 73, 73, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 36, 36, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 17, 17, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 15, 15, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 6272)              25088     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               1605888   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 1,767,682\n",
      "Trainable params: 1,754,050\n",
      "Non-trainable params: 13,632\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1363 samples, validate on 241 samples\n",
      "Epoch 1/15\n",
      "1363/1363 [==============================] - 7s 5ms/step - loss: 0.7682 - acc: 0.6273 - val_loss: 1.4396 - val_acc: 0.5187\n",
      "Epoch 2/15\n",
      "1363/1363 [==============================] - 6s 4ms/step - loss: 0.6494 - acc: 0.6698 - val_loss: 0.4605 - val_acc: 0.7635\n",
      "Epoch 3/15\n",
      "1363/1363 [==============================] - 6s 4ms/step - loss: 0.5607 - acc: 0.7293 - val_loss: 0.5484 - val_acc: 0.6473\n",
      "Epoch 4/15\n",
      "1363/1363 [==============================] - 6s 4ms/step - loss: 0.5138 - acc: 0.7454 - val_loss: 0.4067 - val_acc: 0.8091\n",
      "Epoch 5/15\n",
      "1363/1363 [==============================] - 6s 4ms/step - loss: 0.4564 - acc: 0.7821 - val_loss: 0.6921 - val_acc: 0.6805\n",
      "Epoch 6/15\n",
      "1363/1363 [==============================] - 6s 4ms/step - loss: 0.4107 - acc: 0.7990 - val_loss: 0.3428 - val_acc: 0.8133\n",
      "Epoch 7/15\n",
      "1363/1363 [==============================] - 6s 4ms/step - loss: 0.3974 - acc: 0.8188 - val_loss: 0.5762 - val_acc: 0.6929\n",
      "Epoch 8/15\n",
      "1363/1363 [==============================] - 6s 4ms/step - loss: 0.3842 - acc: 0.8225 - val_loss: 0.3354 - val_acc: 0.8506\n",
      "Epoch 9/15\n",
      "1363/1363 [==============================] - 6s 4ms/step - loss: 0.3432 - acc: 0.8591 - val_loss: 0.5183 - val_acc: 0.6722\n",
      "Epoch 10/15\n",
      "1363/1363 [==============================] - 6s 4ms/step - loss: 0.3391 - acc: 0.8518 - val_loss: 1.3985 - val_acc: 0.5477\n",
      "Epoch 11/15\n",
      "1363/1363 [==============================] - 6s 4ms/step - loss: 0.3554 - acc: 0.8437 - val_loss: 0.4932 - val_acc: 0.7344\n",
      "Epoch 12/15\n",
      "1363/1363 [==============================] - 6s 4ms/step - loss: 0.3226 - acc: 0.8628 - val_loss: 0.6430 - val_acc: 0.5145\n",
      "Epoch 13/15\n",
      "1363/1363 [==============================] - 6s 4ms/step - loss: 0.2956 - acc: 0.8731 - val_loss: 0.3242 - val_acc: 0.8423\n",
      "Epoch 14/15\n",
      "1363/1363 [==============================] - 6s 4ms/step - loss: 0.2656 - acc: 0.8914 - val_loss: 0.3697 - val_acc: 0.8423\n",
      "Epoch 15/15\n",
      "1363/1363 [==============================] - 6s 4ms/step - loss: 0.2679 - acc: 0.8804 - val_loss: 0.3730 - val_acc: 0.8548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fab75916550>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, Y_train,\n",
    "              batch_size=batch_size, validation_data=(X_test, Y_test),\n",
    "              epochs=15,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reaches ~90% fairly quickly, then becomes a bit unstable :/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model1.predict(X_test)\n",
    "for i in range(len(Y_pred)):\n",
    "    Y_pred[i, :] = np.round(Y_pred[i, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AC:0.79 PR:0.71 RE:0.94 F1:0.80\n"
     ]
    }
   ],
   "source": [
    "#Precision: how many 1 are true 1?\n",
    "#Recall: how many 0 are true 0? \n",
    "#F1Score: balance between those two\n",
    "\n",
    "y_comp = confusion_matrix(Y_test[:, 1], Y_pred[:, 1])\n",
    "precision = y_comp[1, 1] / (y_comp[0, 1] + y_comp[1, 1])\n",
    "recall = y_comp[1, 1] / (y_comp[1, 0] + y_comp[1, 1])\n",
    "accuracy = accuracy_score(Y_test[:, 1], Y_pred[:, 1])\n",
    "f1score = 2 * precision * recall / (precision + recall)\n",
    "print(\"AC:%.2f PR:%.2f RE:%.2f F1:%.2f\" % (accuracy, precision, recall, f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename2 = (\"xdddjson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = model1.to_json()\n",
    "with open(filename2, \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
