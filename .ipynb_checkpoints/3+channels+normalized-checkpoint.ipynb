{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "# -----\n",
    "from scipy import signal\n",
    "from skimage.restoration import denoise_tv_chambolle\n",
    "from skimage.filters import gaussian\n",
    "import json\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, ZeroPadding2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def channels(data):\n",
    "    rgb_arrays = []\n",
    "    for i, row in data.iterrows():\n",
    "        \n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = (band_1 + band_2)/2\n",
    "        \n",
    "        band_1 = denoise_tv_chambolle(band_1, weight=0.05) # chosen arbitrarily\n",
    "        band_2 = denoise_tv_chambolle(band_2, weight=0.05)\n",
    "        band_3 = denoise_tv_chambolle(band_3, weight=0.05)\n",
    "        \n",
    "        # gradient magnitude - doesn't seem to work any better than (b1 + b2)/2\n",
    "#         xder = np.array([[-1,0,1],[-2,0,2],[-1,0,1]])\n",
    "#         yder = np.array([[1,2,1],[0,0,0],[-1,-2,-1]])\n",
    "#         arrx = signal.convolve2d(band_1, xder, mode='same')\n",
    "#         arry = signal.convolve2d(band_1, yder, mode='same')\n",
    "#         band_3 = np.hypot(arrx, arry) \n",
    "\n",
    "        rgb = np.dstack((band_1, band_2, band_3))\n",
    "        rgb_arrays.append(rgb)\n",
    "        \n",
    "    rgb_arrays = np.array(rgb_arrays)\n",
    "    for j in range(3):\n",
    "        rgb_arrays[:, :, :, j] = (rgb_arrays[:, :, :, j] - np.mean(rgb_arrays[:, :, :, j]))/np.std(rgb_arrays[:, :, :, j])\n",
    "    return np.array(rgb_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_json('train.json')\n",
    "Y = X['is_iceberg']  # labels\n",
    "\n",
    "X = channels(X)\n",
    "Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1363, 75, 75, 3)\n",
      "(241, 75, 75, 3)\n",
      "(1363, 2)\n",
      "(241, 2)\n"
     ]
    }
   ],
   "source": [
    "image_height = 75\n",
    "image_width = 75\n",
    "channels = 3\n",
    "number_of_classes = 2\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)\n",
    "\n",
    "X_train = channels(X_train)\n",
    "X_test = channels(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "# 1\n",
    "model1.add(Conv2D(32, (3, 3), input_shape=(image_width, image_height, channels)))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 2\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(64, (3, 3)))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 3\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(128, (3, 3)))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 4\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(256, (3, 3)))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model1.add(Flatten())\n",
    "\n",
    "# 0\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(512, use_bias=True, bias_initializer='zeros'))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(Dropout(0.15))\n",
    "\n",
    "# 1\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(256, use_bias=True, bias_initializer='zeros'))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(Dropout(0.15))\n",
    "\n",
    "# 2\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(128, use_bias=True, bias_initializer='zeros'))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(Dropout(0.15))\n",
    "\n",
    "# 3\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(64, use_bias=True, bias_initializer='zeros'))\n",
    "model1.add(LeakyReLU(alpha=.01))\n",
    "model1.add(Dropout(0.15))\n",
    "\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(number_of_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1363 samples, validate on 241 samples\n",
      "Epoch 1/30\n",
      "1363/1363 [==============================] - 11s - loss: 0.6734 - acc: 0.6706 - val_loss: 0.6632 - val_acc: 0.5975\n",
      "Epoch 2/30\n",
      "1363/1363 [==============================] - 9s - loss: 0.5840 - acc: 0.7175 - val_loss: 0.7720 - val_acc: 0.6390\n",
      "Epoch 3/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.4685 - acc: 0.7770 - val_loss: 0.4667 - val_acc: 0.7676\n",
      "Epoch 4/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.4229 - acc: 0.8210 - val_loss: 0.3613 - val_acc: 0.8299\n",
      "Epoch 5/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.3758 - acc: 0.8225 - val_loss: 0.3556 - val_acc: 0.8216\n",
      "Epoch 6/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.4035 - acc: 0.8320 - val_loss: 0.4618 - val_acc: 0.7469\n",
      "Epoch 7/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.3181 - acc: 0.8738 - val_loss: 0.2967 - val_acc: 0.8838\n",
      "Epoch 8/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.3196 - acc: 0.8584 - val_loss: 0.2486 - val_acc: 0.8963\n",
      "Epoch 9/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.2738 - acc: 0.8899 - val_loss: 0.5474 - val_acc: 0.7967\n",
      "Epoch 10/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.2630 - acc: 0.9010 - val_loss: 0.3132 - val_acc: 0.8631\n",
      "Epoch 11/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.3454 - acc: 0.8379 - val_loss: 0.3873 - val_acc: 0.8382\n",
      "Epoch 12/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.3422 - acc: 0.8584 - val_loss: 0.3697 - val_acc: 0.8008\n",
      "Epoch 13/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.4269 - acc: 0.8100 - val_loss: 0.2696 - val_acc: 0.8797\n",
      "Epoch 14/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.3617 - acc: 0.8276 - val_loss: 0.3200 - val_acc: 0.8631\n",
      "Epoch 15/30\n",
      "1363/1363 [==============================] - 10s - loss: 0.2917 - acc: 0.8782 - val_loss: 0.2596 - val_acc: 0.8880\n",
      "Epoch 16/30\n",
      " 592/1363 [============>.................] - ETA: 5s - loss: 0.2772 - acc: 0.8818"
     ]
    }
   ],
   "source": [
    "model1.fit(X_train, Y_train,\n",
    "              batch_size=batch_size, validation_data=(X_test, Y_test),\n",
    "              epochs=epochs,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reaches ~90% fairly quickly, then becomes a bit unstable :/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model1.predict(X_test)\n",
    "for i in range(len(Y_pred)):\n",
    "    Y_pred[i, :] = np.round(Y_pred[i, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AC:0.89 PR:0.82 RE:0.94 F1:0.88\n"
     ]
    }
   ],
   "source": [
    "#Precision: how many 1 are true 1?\n",
    "#Recall: how many 0 are true 0? \n",
    "#F1Score: balance between those two\n",
    "\n",
    "y_comp = confusion_matrix(Y_test[:, 1], Y_pred[:, 1])\n",
    "precision = y_comp[1, 1] / (y_comp[0, 1] + y_comp[1, 1])\n",
    "recall = y_comp[1, 1] / (y_comp[1, 0] + y_comp[1, 1])\n",
    "accuracy = accuracy_score(Y_test[:, 1], Y_pred[:, 1])\n",
    "f1score = 2 * precision * recall / (precision + recall)\n",
    "print(\"AC:%.2f PR:%.2f RE:%.2f F1:%.2f\" % (accuracy, precision, recall, f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = (\"model_%i_%i_%i_%i.h5\" % (accuracy*100, precision*100, recall*100, f1score*100))\n",
    "filename2 = (\"model_%i_%i_%i_%i.json\" % (accuracy*100, precision*100, recall*100, f1score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = model1.to_json()\n",
    "with open(filename2, \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
