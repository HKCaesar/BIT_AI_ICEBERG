{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# turning off my piece of fucking shit GPU\n",
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "# -----\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, ZeroPadding2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As for the data\n",
    "133 out of 1603 examples are missing on 'inc_angle' value.  \n",
    "#### All of them are ships!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_temp = pd.read_json('train.json')\n",
    "sum = 0\n",
    "list = []\n",
    "for i in range(len(data_temp['inc_angle'])):\n",
    "    if data_temp['inc_angle'][i] == \"na\":\n",
    "        sum += 1\n",
    "        list.append(i)\n",
    "print(\"Number of datapoints with missing \\'inc_angle\\':\", sum, \"\\n\\n\")\n",
    "print(\"List of indices:\\n\\n\", list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First half-hearted attempt\n",
    "## Data and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def channels(data):\n",
    "    rgb_arrays = []\n",
    "    for i, row in data.iterrows():\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 + band_2  # maybe divide instead of adding?\n",
    "\n",
    "        rgb = np.dstack((band_1, band_2, band_3))\n",
    "        rgb_arrays.append(rgb)\n",
    "    return np.array(rgb_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_json('train.json')\n",
    "Y = X['is_iceberg']  # labels\n",
    "\n",
    "X = channels(X)\n",
    "Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1363, 75, 75, 3)\n",
      "(241, 75, 75, 3)\n",
      "(1363, 2)\n",
      "(241, 2)\n"
     ]
    }
   ],
   "source": [
    "image_height = 75\n",
    "image_width = 75\n",
    "channels = 3\n",
    "number_of_classes = 2\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_width, image_height, channels)))\n",
    "\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model1.add(Flatten())\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(256, activation='relu', use_bias=True, bias_initializer='zeros'))\n",
    "\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(number_of_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1363 samples, validate on 241 samples\n",
      "Epoch 1/10\n",
      "1363/1363 [==============================] - 213s - loss: 1.3020 - acc: 0.6647 - val_loss: 6.8422 - val_acc: 0.5062\n",
      "Epoch 2/10\n",
      "1363/1363 [==============================] - 210s - loss: 0.4396 - acc: 0.7902 - val_loss: 2.8130 - val_acc: 0.4315\n",
      "Epoch 3/10\n",
      "1363/1363 [==============================] - 211s - loss: 0.3737 - acc: 0.8335 - val_loss: 0.9174 - val_acc: 0.5062\n",
      "Epoch 4/10\n",
      "1363/1363 [==============================] - 213s - loss: 0.3135 - acc: 0.8665 - val_loss: 0.6162 - val_acc: 0.6141\n",
      "Epoch 5/10\n",
      "1363/1363 [==============================] - 168s - loss: 0.2659 - acc: 0.8804 - val_loss: 0.9683 - val_acc: 0.5145\n",
      "Epoch 6/10\n",
      "1363/1363 [==============================] - 164s - loss: 0.2434 - acc: 0.9024 - val_loss: 2.0829 - val_acc: 0.5602\n",
      "Epoch 7/10\n",
      "1363/1363 [==============================] - 169s - loss: 0.1971 - acc: 0.9222 - val_loss: 0.4889 - val_acc: 0.7552\n",
      "Epoch 8/10\n",
      "1363/1363 [==============================] - 165s - loss: 0.1618 - acc: 0.9435 - val_loss: 0.7696 - val_acc: 0.6432\n",
      "Epoch 9/10\n",
      "1363/1363 [==============================] - 166s - loss: 0.1540 - acc: 0.9413 - val_loss: 2.9714 - val_acc: 0.5021\n",
      "Epoch 10/10\n",
      "1363/1363 [==============================] - 170s - loss: 0.0923 - acc: 0.9648 - val_loss: 0.3681 - val_acc: 0.8382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f274a873c88>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, Y_train,\n",
    "              batch_size=batch_size, validation_data=(X_test, Y_test),\n",
    "              epochs=epochs,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1363 samples, validate on 241 samples\n",
      "Epoch 1/5\n",
      "1363/1363 [==============================] - 184s - loss: 0.1087 - acc: 0.9567 - val_loss: 1.5829 - val_acc: 0.6556\n",
      "Epoch 2/5\n",
      "1363/1363 [==============================] - 167s - loss: 0.1076 - acc: 0.9611 - val_loss: 0.3916 - val_acc: 0.8299\n",
      "Epoch 3/5\n",
      "1363/1363 [==============================] - 166s - loss: 0.1097 - acc: 0.9626 - val_loss: 1.3985 - val_acc: 0.6390\n",
      "Epoch 4/5\n",
      "1363/1363 [==============================] - 167s - loss: 0.0995 - acc: 0.9633 - val_loss: 1.8702 - val_acc: 0.7054\n",
      "Epoch 5/5\n",
      "1363/1363 [==============================] - 168s - loss: 0.0962 - acc: 0.9677 - val_loss: 0.4986 - val_acc: 0.7635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2762856a90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, Y_train,\n",
    "              batch_size=batch_size, validation_data=(X_test, Y_test),\n",
    "              epochs=5,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convergence is fucking shit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
